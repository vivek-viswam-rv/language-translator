{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgnEkaprXi6e"
      },
      "source": [
        "# Language translator: English to French\n",
        "\n",
        "Vivek Viswam R. V. <br>\n",
        "Rachel Messenger"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "R5CIbK63DVsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVQIQ8jbIKuz",
        "outputId": "c926b27f-7615-4de9-df68-12ecd5590fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "!python -m pip install torchmetrics\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from torchmetrics.text import BLEUScore\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AynL4IUlG-2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5dead96-9c96-4c0f-8649-cb0958825fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constants"
      ],
      "metadata": {
        "id": "mQWBise3DTw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X5wz1uDTfey"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = \"/content/drive/MyDrive/language-translator\"\n",
        "EN_REL_PATH = \"dataset/europarl-v7.fr-en.en\"\n",
        "FR_REL_PATH = \"dataset/europarl-v7.fr-en.fr\"\n",
        "TRAIN_STATS_FILENAME = \"training_stats.pth\"\n",
        "\n",
        "EN_PATH = os.path.join(ROOT_PATH, EN_REL_PATH)\n",
        "FR_PATH = os.path.join(ROOT_PATH, FR_REL_PATH)\n",
        "TRAIN_STATS_PATH = os.path.join(ROOT_PATH, TRAIN_STATS_FILENAME)\n",
        "\n",
        "START_TOKEN = \"<START>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "UNKN_TOKEN = \"<UNKN>\"\n",
        "\n",
        "MAX_SENT_LEN = 100\n",
        "MAX_SEQ_LEN = MAX_SENT_LEN + 2\n",
        "MAX_LINE_COUNT = 200000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom dataset class definitions"
      ],
      "metadata": {
        "id": "IOitEOMSDcr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LangDataset(Dataset):\n",
        "  START_TOKEN = START_TOKEN\n",
        "  END_TOKEN = END_TOKEN\n",
        "  PAD_TOKEN = PAD_TOKEN\n",
        "  UNKN_TOKEN = UNKN_TOKEN\n",
        "  SPECIAL_TOKENS = ( START_TOKEN, END_TOKEN, PAD_TOKEN, UNKN_TOKEN )\n",
        "\n",
        "  def __init__(self, src_path, tgt_path, src_tokenizer, tgt_tokenizer, min_tok_freq=2, max_lines=None, max_sent_len=MAX_SENT_LEN):\n",
        "    self.src = None\n",
        "    self.tgt = None\n",
        "\n",
        "    self.src_path = src_path\n",
        "    self.tgt_path = tgt_path\n",
        "    self.max_sent_len = max_sent_len\n",
        "    self.max_seq_len = max_sent_len + 2\n",
        "    self.min_tok_freq = min_tok_freq\n",
        "\n",
        "    self.src_lines = []\n",
        "    self.tgt_lines = []\n",
        "    self.src_idx2tok = {}\n",
        "    self.src_tok2idx = {}\n",
        "    self.tgt_idx2tok = {}\n",
        "    self.tgt_tok2idx = {}\n",
        "    self.src_vocab_size = 0\n",
        "    self.tgt_vocab_size = 0\n",
        "\n",
        "    self._src_tokenizer = src_tokenizer\n",
        "    self._tgt_tokenizer = tgt_tokenizer\n",
        "    self._max_lines = max_lines\n",
        "\n",
        "\n",
        "    self._load_dataset()\n",
        "    self._preprocess_dataset()\n",
        "    self._create_vocab()\n",
        "    self._tensorify_dataset()\n",
        "\n",
        "  def tokenize_sentence(self, sentence, lang_type=\"src\"):\n",
        "      tokenize = self._src_tokenizer if lang_type == \"src\" else self._tgt_tokenizer\n",
        "      tok2idx = self.src_tok2idx if lang_type == \"src\" else self.tgt_tok2idx\n",
        "      counter = self.src_tok_ctr if lang_type == \"src\" else self.tgt_tok_ctr\n",
        "\n",
        "      tokens = tokenize(sentence)\n",
        "      tokens = [ self.UNKN_TOKEN if counter[tok] < self.min_tok_freq else tok for tok in tokens ]\n",
        "      tokens = [ self.START_TOKEN ] + tokens + [ self.END_TOKEN ]\n",
        "      padding_size = self.max_seq_len - len(tokens)\n",
        "      tokens += [ self.PAD_TOKEN ] * padding_size\n",
        "      tokens = [ tok2idx[tok] for tok in tokens ]\n",
        "\n",
        "      return tokens\n",
        "\n",
        "  def detokenize_tensor(self, tensor, lang_type=\"src\"):\n",
        "      idx2tok = self.src_idx2tok if lang_type == \"src\" else self.tgt_idx2tok\n",
        "      words = [ idx2tok[idx.item()] for idx in tensor ]\n",
        "\n",
        "      return words\n",
        "\n",
        "  def _load_dataset(self):\n",
        "    print(\"Reading source and target files...\")\n",
        "\n",
        "    with open(self.src_path) as src_f, open(self.tgt_path) as tgt_path:\n",
        "      src_lines = src_f.read().splitlines()\n",
        "      tgt_lines = tgt_path.read().splitlines()\n",
        "\n",
        "      self.src_lines = src_lines[:self._max_lines] if self._max_lines else src_lines\n",
        "      self.tgt_lines = tgt_lines[:self._max_lines] if self._max_lines else tgt_lines\n",
        "\n",
        "    print(f\"Reading source files! Read {len(src_lines)} lines. Keeping {len(self.src_lines)} lines.\")\n",
        "\n",
        "  def _preprocess_dataset(self):\n",
        "    print(\"Preprocessing dataset...\")\n",
        "\n",
        "    final_src_lines = []\n",
        "    final_tgt_lines = []\n",
        "    total_lines = len(self.src_lines)\n",
        "\n",
        "    self.src_tok_ctr = Counter()\n",
        "    self.tgt_tok_ctr = Counter()\n",
        "\n",
        "    self._src_tok_set = set()\n",
        "    self._tgt_tok_set = set()\n",
        "\n",
        "    for idx in range(total_lines):\n",
        "      src_line = self.src_lines[idx].lower()\n",
        "      tgt_line = self.tgt_lines[idx].lower()\n",
        "\n",
        "      src_line_toks = self._src_tokenizer(src_line)\n",
        "      tgt_line_toks = self._tgt_tokenizer(tgt_line)\n",
        "\n",
        "      src_line_len = len(src_line_toks)\n",
        "      tgt_line_len = len(tgt_line_toks)\n",
        "\n",
        "      if src_line_len <= self.max_sent_len and tgt_line_len <= self.max_sent_len:\n",
        "        final_src_lines.append(src_line)\n",
        "        final_tgt_lines.append(tgt_line)\n",
        "\n",
        "        self._src_tok_set |= set(src_line_toks)\n",
        "        self._tgt_tok_set |= set(tgt_line_toks)\n",
        "\n",
        "        self.src_tok_ctr.update(src_line_toks)\n",
        "        self.tgt_tok_ctr.update(tgt_line_toks)\n",
        "\n",
        "    self.src_lines, self.tgt_lines = final_src_lines, final_tgt_lines\n",
        "\n",
        "    print(f\"Preprocessing completed! Dropped {total_lines - len(self.src_lines)}/{total_lines} lines!\")\n",
        "\n",
        "  def _create_vocab(self):\n",
        "    print(\"Creating vocabularies...\")\n",
        "\n",
        "    src_tokens = list(self.SPECIAL_TOKENS) + list(self._src_tok_set)\n",
        "    tgt_tokens = list(self.SPECIAL_TOKENS) + list(self._tgt_tok_set)\n",
        "\n",
        "    del self._src_tok_set\n",
        "    del self._tgt_tok_set\n",
        "\n",
        "    for idx, tok in enumerate(src_tokens):\n",
        "      self.src_tok2idx[tok] = idx\n",
        "      self.src_idx2tok[idx] = tok\n",
        "\n",
        "    for idx, tok in enumerate(tgt_tokens):\n",
        "      self.tgt_tok2idx[tok] = idx\n",
        "      self.tgt_idx2tok[idx] = tok\n",
        "\n",
        "    self.src_vocab_size = len(self.src_tok2idx)\n",
        "    self.tgt_vocab_size = len(self.tgt_tok2idx)\n",
        "\n",
        "    print(f\"Created vocabularies! Source vocab size: {self.src_vocab_size} and target vocab size: {self.tgt_vocab_size}\")\n",
        "\n",
        "  def _tensorify_dataset(self):\n",
        "    print(\"Tensorifying dataset...\")\n",
        "\n",
        "    final_src_lines = [ self.tokenize_sentence(line, \"src\") for line in self.src_lines ]\n",
        "    final_tgt_lines = [ self.tokenize_sentence(line, \"tgt\") for line in self.tgt_lines ]\n",
        "\n",
        "    self.src = torch.tensor(final_src_lines)\n",
        "    self.tgt = torch.tensor(final_tgt_lines)\n",
        "\n",
        "    print(\"Dataset tensorified!\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.src[idx], self.tgt[idx]"
      ],
      "metadata": {
        "id": "lFqTteiWVxp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class definitions"
      ],
      "metadata": {
        "id": "DFol28NEDl3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_size,\n",
        "               num_heads,\n",
        "               num_encoder_layers,\n",
        "               num_decoder_layers,\n",
        "               dim_feedforward,\n",
        "               dropout,\n",
        "               src_voc_size,\n",
        "               tgt_voc_size,\n",
        "               src_pad_idx,\n",
        "               tgt_pad_idx,\n",
        "               device,\n",
        "               max_sent_len=MAX_SENT_LEN,\n",
        "    ):\n",
        "\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.device = device\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.tgt_pad_idx = tgt_pad_idx\n",
        "    self.max_seq_len = max_sent_len + 2\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.src_word_embedding = nn.Embedding(src_voc_size, embedding_size)\n",
        "    self.src_position_embedding = nn.Embedding(self.max_seq_len, embedding_size)\n",
        "    self.tgt_word_embedding = nn.Embedding(tgt_voc_size, embedding_size)\n",
        "    self.tgt_position_embedding = nn.Embedding(self.max_seq_len, embedding_size)\n",
        "    self.transformer = nn.Transformer(\n",
        "        embedding_size,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dim_feedforward,\n",
        "        dropout\n",
        "    )\n",
        "\n",
        "    self.fc_out = nn.Linear(embedding_size, tgt_voc_size)\n",
        "\n",
        "  def create_padding_mask(self, seq, pad_idx):\n",
        "    return seq.transpose(0, 1) == pad_idx\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    src_seq_len, batch_size = src.shape\n",
        "    tgt_seq_len, _ = tgt.shape\n",
        "\n",
        "    src_positions = (\n",
        "        torch.arange(0, src_seq_len).unsqueeze(1).expand(src_seq_len, batch_size)\n",
        "        .to(self.device)\n",
        "    )\n",
        "\n",
        "    tgt_positions = (\n",
        "        torch.arange(0, tgt_seq_len).unsqueeze(1).expand(tgt_seq_len, batch_size)\n",
        "        .to(self.device)\n",
        "    )\n",
        "\n",
        "    emb_src = self.dropout(\n",
        "        self.src_word_embedding(src) + self.src_position_embedding(src_positions)\n",
        "    )\n",
        "\n",
        "    emb_tgt = self.dropout(\n",
        "        self.tgt_word_embedding(tgt) + self.tgt_position_embedding(tgt_positions)\n",
        "    )\n",
        "\n",
        "    src_padding_mask = self.create_padding_mask(src, self.src_pad_idx)\n",
        "    tgt_padding_mask = self.create_padding_mask(tgt, self.tgt_pad_idx)\n",
        "    tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len).to(self.device)\n",
        "\n",
        "    out = self.transformer(\n",
        "        emb_src,\n",
        "        emb_tgt,\n",
        "        tgt_mask=tgt_mask,\n",
        "        src_key_padding_mask=src_padding_mask,\n",
        "        tgt_key_padding_mask=tgt_padding_mask\n",
        "    )\n",
        "\n",
        "    return self.fc_out(out)"
      ],
      "metadata": {
        "id": "Kpkl5o8q999f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions"
      ],
      "metadata": {
        "id": "YbI0Sts37xUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_translate_tensor(model, batch_data, max_length=MAX_SEQ_LEN, detokenize=True):\n",
        "  data_len, seq_len = batch_data.shape\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  src = batch_data.T.to(device)\n",
        "  tgt = torch.full(\n",
        "      (1, data_len),\n",
        "      dataset.tgt_tok2idx[START_TOKEN],\n",
        "      dtype=torch.long,\n",
        "      device=device\n",
        "  )\n",
        "\n",
        "  finished = torch.zeros(data_len, dtype=torch.bool, device=device)\n",
        "\n",
        "  for _ in range(max_length-1):\n",
        "    with torch.no_grad():\n",
        "      output = model(src, tgt)\n",
        "\n",
        "    next_token_logits = output[-1, :, :]\n",
        "    next_tokens = next_token_logits.argmax(dim=1)\n",
        "\n",
        "    next_tokens = next_tokens.masked_fill(finished, dataset.src_tok2idx[END_TOKEN])\n",
        "    tgt = torch.cat([tgt, next_tokens.unsqueeze(0)], dim=0)\n",
        "\n",
        "    finished |= (next_tokens == dataset.src_tok2idx[END_TOKEN])\n",
        "    if finished.all():\n",
        "      break\n",
        "\n",
        "  if not detokenize:\n",
        "    return tgt.T\n",
        "\n",
        "  translations = []\n",
        "  tgt_np = tgt.detach().cpu().numpy().T\n",
        "\n",
        "  for seq in tgt_np:\n",
        "    words = []\n",
        "    for idx in seq[1:]:\n",
        "      if idx == dataset.tgt_tok2idx[END_TOKEN]:\n",
        "        break\n",
        "      words.append(dataset.tgt_idx2tok[idx])\n",
        "    translations.append(\" \".join(words))\n",
        "\n",
        "  return translations\n",
        "\n",
        "def en_token_rejoin(tokens):\n",
        "  sentence = \"\"\n",
        "  for token in tokens:\n",
        "    if token[0] in [\"'\", \".\", \",\", \"!\", \"?\", \":\", \";\"]:\n",
        "      sentence = sentence.rstrip() + token\n",
        "      continue\n",
        "\n",
        "    sentence+= token + \" \"\n",
        "\n",
        "  return sentence\n",
        "\n",
        "def translate_sentence(model, sentence, max_length=MAX_SENT_LEN):\n",
        "  tokens = [token.text.lower() for token in spacy_fr(sentence)]\n",
        "\n",
        "  tokens.insert(0, START_TOKEN)\n",
        "  tokens.append(END_TOKEN)\n",
        "\n",
        "  text_to_indices = [ dataset.src_tok2idx.get(token, dataset.src_tok2idx[UNKN_TOKEN]) for token in tokens]\n",
        "  sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "  outputs = [dataset.tgt_tok2idx[START_TOKEN]]\n",
        "  for i in range(max_length):\n",
        "      tgt_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output = model(sentence_tensor, tgt_tensor)\n",
        "\n",
        "      best_guess = output.argmax(2)[-1, :].item()\n",
        "      if best_guess == dataset.tgt_tok2idx[END_TOKEN]:\n",
        "          break\n",
        "\n",
        "      outputs.append(best_guess)\n",
        "\n",
        "  translated_sentence = [dataset.tgt_idx2tok[idx] for idx in outputs]\n",
        "\n",
        "  return en_token_rejoin(translated_sentence[1:])\n",
        "\n",
        "def bleu_score(model, test_iter, max_count=None, verbose=True):\n",
        "  tgts = []\n",
        "  pred_tgts = []\n",
        "  cnt = 0\n",
        "\n",
        "  for (x, y) in test_iter:\n",
        "    src = x.to(device)\n",
        "    tgt = y[:, :-1]\n",
        "\n",
        "    pred = batch_translate_tensor(model, src)\n",
        "    pred_tgts.extend(pred)\n",
        "    for sent_tok in tgt:\n",
        "      tgts.append([\" \".join([ val for tok in sent_tok if (val:=dataset.tgt_idx2tok[tok.item()]) not in [START_TOKEN, END_TOKEN, PAD_TOKEN]])])\n",
        "\n",
        "    cnt += src.shape[0]\n",
        "    if max_count and cnt >= max_count:\n",
        "      break\n",
        "\n",
        "  bleu = BLEUScore().to(device)\n",
        "  score = bleu(pred_tgts, tgts)\n",
        "\n",
        "  if(verbose):\n",
        "    print(f\"Total tested datapoints: {cnt}\")\n",
        "    print(f\"BLEU score: {score * 100:.2f}%\")\n",
        "\n",
        "  return score.item()"
      ],
      "metadata": {
        "id": "gzVbYR-dCh8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misc definitions"
      ],
      "metadata": {
        "id": "2Y9twgDBxVSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "tokenize_fr = lambda text: [ text for tok in spacy_fr.tokenizer(text) if (text:=tok.text.strip()) ]\n",
        "tokenize_en = lambda text: [ text for tok in spacy_en.tokenizer(text) if (text:=tok.text.strip()) ]"
      ],
      "metadata": {
        "id": "IN7LG1wNxUWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading dataset and vocabularies"
      ],
      "metadata": {
        "id": "zq3_cbXODoEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqMsi5lk6EKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd7f393-6c39-4ff3-e920-f5d38dcc9ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading source and target files...\n",
            "Reading source files! Read 2007723 lines. Keeping 200000 lines.\n",
            "Preprocessing dataset...\n",
            "Preprocessing completed! Dropped 1317/200000 lines!\n",
            "Creating vocabularies...\n",
            "Created vocabularies! Source vocab size: 52869 and target vocab size: 37116\n",
            "Tensorifying dataset...\n",
            "Dataset tensorified!\n"
          ]
        }
      ],
      "source": [
        "batch_size = 512\n",
        "train_test_split = [0.9, 0.1]\n",
        "\n",
        "dataset = LangDataset(FR_PATH, EN_PATH, tokenize_fr, tokenize_en, max_lines=MAX_LINE_COUNT)\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(dataset, train_test_split)\n",
        "train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_iter = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "Zeg7Z1biDzm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_iter, val_iter, dataset, device, config):\n",
        "  embedding_size = config[\"embedding_size\"]\n",
        "  num_heads = config[\"num_heads\"]\n",
        "  num_encoder_layers = config[\"num_encoder_layers\"]\n",
        "  num_decoder_layers = config[\"num_decoder_layers\"]\n",
        "  dropout = config[\"dropout\"]\n",
        "  learning_rate = config[\"learning_rate\"]\n",
        "  num_epochs = config[\"num_epochs\"]\n",
        "\n",
        "  log_per = 1\n",
        "  dim_feedforward = 4 * embedding_size\n",
        "\n",
        "  src_pad_idx = dataset.src_tok2idx[PAD_TOKEN]\n",
        "  tgt_pad_idx = dataset.tgt_tok2idx[PAD_TOKEN]\n",
        "  tgt_start_idx = dataset.tgt_tok2idx[START_TOKEN]\n",
        "  tgt_end_idx = dataset.tgt_tok2idx[END_TOKEN]\n",
        "\n",
        "  model = Transformer(\n",
        "    embedding_size,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    dim_feedforward,\n",
        "    dropout,\n",
        "    dataset.src_vocab_size,\n",
        "    dataset.tgt_vocab_size,\n",
        "    src_pad_idx,\n",
        "    tgt_pad_idx,\n",
        "    device\n",
        "  ).to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
        "\n",
        "  loss_history = []\n",
        "  acc_history = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    batch_loss = []\n",
        "    batch_acc = []\n",
        "\n",
        "    model.train()\n",
        "    for src, target in train_iter:\n",
        "      src = src.to(device).T\n",
        "      target = target.to(device).T\n",
        "\n",
        "      target_input  = target[:-1, :]\n",
        "      target_output = target[1:, :]\n",
        "\n",
        "      output = model(src, target_input)\n",
        "      pred = output.argmax(2)\n",
        "\n",
        "      mask = (\n",
        "        (target_output != tgt_end_idx) &\n",
        "        (target_output != tgt_pad_idx)\n",
        "      )\n",
        "\n",
        "      correct  = (pred == target_output) & mask\n",
        "      accuracy = correct.sum() / mask.sum()\n",
        "      batch_acc.append(accuracy.item())\n",
        "\n",
        "      output_flat = output.reshape(-1, output.shape[2])\n",
        "      target_flat = target_output.reshape(-1)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(output_flat, target_flat)\n",
        "      loss.backward()\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "      optimizer.step()\n",
        "\n",
        "    avg_acc  = sum(batch_acc) / len(batch_acc)\n",
        "    avg_loss = sum(batch_loss) / len(batch_loss)\n",
        "    acc_history.append(avg_acc)\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    torch.save({\n",
        "      \"loss_history\": loss_history,\n",
        "      \"acc_history\": acc_history,\n",
        "      \"epoch\": epoch,\n",
        "      \"datapoints\": MAX_LINE_COUNT,\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      }, TRAIN_STATS_PATH)\n",
        "\n",
        "    if epoch % log_per == 0:\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} - loss: {avg_loss:.4f} - train acc: {avg_acc * 100:.2f}%\")\n",
        "\n",
        "  test_bleu = bleu_score(model, test_iter, verbose=False)\n",
        "  print(f\"Epoch {epoch + 1}/{num_epochs} - loss: {avg_loss:.4f} - train acc: {avg_acc * 100:.2f}%\")\n",
        "  print(f\"Test BLEU: {test_bleu * 100:.2f}%\")\n",
        "\n",
        "  torch.save({\n",
        "  \"loss_history\": loss_history,\n",
        "  \"acc_history\": acc_history,\n",
        "  \"epoch\": epoch,\n",
        "  \"datapoints\": MAX_LINE_COUNT,\n",
        "  \"model_state_dict\": model.state_dict(),\n",
        "  \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "  \"test_bleu\": test_bleu,\n",
        "  }, TRAIN_STATS_PATH)\n",
        "\n",
        "  return test_bleu, model, loss_history, acc_history"
      ],
      "metadata": {
        "id": "AB4yrRVK-mth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "  \"embedding_size\": 512,\n",
        "  \"num_heads\": 4,\n",
        "  \"num_encoder_layers\": 2,\n",
        "  \"num_decoder_layers\": 3,\n",
        "  \"dropout\": 0.1,\n",
        "  \"learning_rate\": 3e-4,\n",
        "  \"num_epochs\":100\n",
        "}\n",
        "\n",
        "test_bleu, model, loss_hist, acc_hist = train(\n",
        "    train_iter, test_iter, dataset, device, config\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp-K4nQG_DkP",
        "outputId": "5beccca6-8c57-435f-c851-ae07f7662e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - loss: 5.0561 - train acc: 22.74%\n",
            "Epoch 2/100 - loss: 3.8038 - train acc: 35.29%\n",
            "Epoch 3/100 - loss: 3.2336 - train acc: 41.89%\n",
            "Epoch 4/100 - loss: 2.8721 - train acc: 46.13%\n",
            "Epoch 5/100 - loss: 2.6304 - train acc: 48.96%\n",
            "Epoch 6/100 - loss: 2.4537 - train acc: 51.01%\n",
            "Epoch 7/100 - loss: 2.3153 - train acc: 52.68%\n",
            "Epoch 8/100 - loss: 2.2020 - train acc: 54.05%\n",
            "Epoch 9/100 - loss: 2.1083 - train acc: 55.20%\n",
            "Epoch 10/100 - loss: 2.0270 - train acc: 56.26%\n",
            "Epoch 11/100 - loss: 1.9581 - train acc: 57.13%\n",
            "Epoch 12/100 - loss: 1.8959 - train acc: 57.98%\n",
            "Epoch 13/100 - loss: 1.8404 - train acc: 58.73%\n",
            "Epoch 14/100 - loss: 1.7902 - train acc: 59.43%\n",
            "Epoch 15/100 - loss: 1.7454 - train acc: 60.06%\n",
            "Epoch 16/100 - loss: 1.7037 - train acc: 60.65%\n",
            "Epoch 17/100 - loss: 1.6647 - train acc: 61.23%\n",
            "Epoch 18/100 - loss: 1.6304 - train acc: 61.72%\n",
            "Epoch 19/100 - loss: 1.5978 - train acc: 62.18%\n",
            "Epoch 20/100 - loss: 1.5659 - train acc: 62.66%\n",
            "Epoch 21/100 - loss: 1.5372 - train acc: 63.10%\n",
            "Epoch 22/100 - loss: 1.5102 - train acc: 63.50%\n",
            "Epoch 23/100 - loss: 1.4848 - train acc: 63.92%\n",
            "Epoch 24/100 - loss: 1.4606 - train acc: 64.27%\n",
            "Epoch 25/100 - loss: 1.4379 - train acc: 64.62%\n",
            "Epoch 26/100 - loss: 1.4156 - train acc: 64.98%\n",
            "Epoch 27/100 - loss: 1.3954 - train acc: 65.27%\n",
            "Epoch 28/100 - loss: 1.3766 - train acc: 65.59%\n",
            "Epoch 29/100 - loss: 1.3569 - train acc: 65.90%\n",
            "Epoch 30/100 - loss: 1.3391 - train acc: 66.19%\n",
            "Epoch 31/100 - loss: 1.3226 - train acc: 66.47%\n",
            "Epoch 32/100 - loss: 1.3048 - train acc: 66.76%\n",
            "Epoch 33/100 - loss: 1.2904 - train acc: 66.98%\n",
            "Epoch 34/100 - loss: 1.2751 - train acc: 67.26%\n",
            "Epoch 35/100 - loss: 1.2603 - train acc: 67.53%\n",
            "Epoch 36/100 - loss: 1.2459 - train acc: 67.77%\n",
            "Epoch 37/100 - loss: 1.2320 - train acc: 68.01%\n",
            "Epoch 38/100 - loss: 1.2186 - train acc: 68.23%\n",
            "Epoch 39/100 - loss: 1.2070 - train acc: 68.42%\n",
            "Epoch 40/100 - loss: 1.1948 - train acc: 68.64%\n",
            "Epoch 41/100 - loss: 1.1829 - train acc: 68.85%\n",
            "Epoch 42/100 - loss: 1.1716 - train acc: 69.05%\n",
            "Epoch 43/100 - loss: 1.1610 - train acc: 69.24%\n",
            "Epoch 44/100 - loss: 1.1501 - train acc: 69.42%\n",
            "Epoch 45/100 - loss: 1.1391 - train acc: 69.63%\n",
            "Epoch 46/100 - loss: 1.1289 - train acc: 69.78%\n",
            "Epoch 47/100 - loss: 1.1192 - train acc: 69.97%\n",
            "Epoch 48/100 - loss: 1.1099 - train acc: 70.15%\n",
            "Epoch 49/100 - loss: 1.1010 - train acc: 70.33%\n",
            "Epoch 50/100 - loss: 1.0920 - train acc: 70.49%\n",
            "Epoch 51/100 - loss: 1.0831 - train acc: 70.65%\n",
            "Epoch 52/100 - loss: 1.0749 - train acc: 70.81%\n",
            "Epoch 53/100 - loss: 1.0660 - train acc: 70.97%\n",
            "Epoch 54/100 - loss: 1.0587 - train acc: 71.11%\n",
            "Epoch 55/100 - loss: 1.0499 - train acc: 71.27%\n",
            "Epoch 56/100 - loss: 1.0419 - train acc: 71.42%\n",
            "Epoch 57/100 - loss: 1.0355 - train acc: 71.54%\n",
            "Epoch 58/100 - loss: 1.0280 - train acc: 71.67%\n",
            "Epoch 59/100 - loss: 1.0204 - train acc: 71.84%\n",
            "Epoch 60/100 - loss: 1.0131 - train acc: 71.96%\n",
            "Epoch 61/100 - loss: 1.0065 - train acc: 72.11%\n",
            "Epoch 62/100 - loss: 0.9997 - train acc: 72.22%\n",
            "Epoch 63/100 - loss: 0.9929 - train acc: 72.38%\n",
            "Epoch 64/100 - loss: 0.9860 - train acc: 72.52%\n",
            "Epoch 65/100 - loss: 0.9802 - train acc: 72.61%\n",
            "Epoch 66/100 - loss: 0.9745 - train acc: 72.73%\n",
            "Epoch 67/100 - loss: 0.9683 - train acc: 72.83%\n",
            "Epoch 68/100 - loss: 0.9624 - train acc: 72.98%\n",
            "Epoch 69/100 - loss: 0.9577 - train acc: 73.06%\n",
            "Epoch 70/100 - loss: 0.9506 - train acc: 73.24%\n",
            "Epoch 71/100 - loss: 0.9452 - train acc: 73.31%\n",
            "Epoch 72/100 - loss: 0.9403 - train acc: 73.42%\n",
            "Epoch 73/100 - loss: 0.9345 - train acc: 73.54%\n",
            "Epoch 74/100 - loss: 0.9290 - train acc: 73.65%\n",
            "Epoch 75/100 - loss: 0.9241 - train acc: 73.75%\n",
            "Epoch 76/100 - loss: 0.9186 - train acc: 73.86%\n",
            "Epoch 77/100 - loss: 0.9139 - train acc: 73.96%\n",
            "Epoch 78/100 - loss: 0.9085 - train acc: 74.06%\n",
            "Epoch 79/100 - loss: 0.9042 - train acc: 74.16%\n",
            "Epoch 80/100 - loss: 0.8995 - train acc: 74.27%\n",
            "Epoch 81/100 - loss: 0.8956 - train acc: 74.34%\n",
            "Epoch 82/100 - loss: 0.8904 - train acc: 74.44%\n",
            "Epoch 83/100 - loss: 0.8857 - train acc: 74.56%\n",
            "Epoch 84/100 - loss: 0.8812 - train acc: 74.62%\n",
            "Epoch 85/100 - loss: 0.8769 - train acc: 74.73%\n",
            "Epoch 86/100 - loss: 0.8726 - train acc: 74.84%\n",
            "Epoch 87/100 - loss: 0.8684 - train acc: 74.92%\n",
            "Epoch 88/100 - loss: 0.8641 - train acc: 75.01%\n",
            "Epoch 89/100 - loss: 0.8599 - train acc: 75.10%\n",
            "Epoch 90/100 - loss: 0.8564 - train acc: 75.16%\n",
            "Epoch 91/100 - loss: 0.8518 - train acc: 75.27%\n",
            "Epoch 92/100 - loss: 0.8478 - train acc: 75.37%\n",
            "Epoch 93/100 - loss: 0.8446 - train acc: 75.41%\n",
            "Epoch 94/100 - loss: 0.8400 - train acc: 75.53%\n",
            "Epoch 95/100 - loss: 0.8370 - train acc: 75.57%\n",
            "Epoch 96/100 - loss: 0.8330 - train acc: 75.69%\n",
            "Epoch 97/100 - loss: 0.8293 - train acc: 75.78%\n",
            "Epoch 98/100 - loss: 0.8258 - train acc: 75.85%\n",
            "Epoch 99/100 - loss: 0.8223 - train acc: 75.92%\n",
            "Epoch 100/100 - loss: 0.8180 - train acc: 76.00%\n",
            "Epoch 100/100 - loss: 0.8180 - train acc: 76.00%\n",
            "Test BLEU: 28.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "  \"src_idx2tok\": dataset.src_idx2tok,\n",
        "  \"src_tok2idx\": dataset.src_tok2idx,\n",
        "  \"tgt_idx2tok\": dataset.tgt_idx2tok,\n",
        "  \"tgt_tok2idx\": dataset.tgt_tok2idx,\n",
        "\n",
        "  }, os.path.join(ROOT_PATH, \"vocab.pth\"))"
      ],
      "metadata": {
        "id": "IN45BAyRNFUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = \"je suis\"\n",
        "actual_translation = \"I am in the process of\"\n",
        "model.eval()\n",
        "prediction = translate_sentence(model, input_sent)\n",
        "print(f\"Input sentence: {input_sent}\")\n",
        "print(f\"Translated sentence: {prediction}\")\n",
        "print(f\"Actual translation: {actual_translation}\")"
      ],
      "metadata": {
        "id": "4EauHauzOIjp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}