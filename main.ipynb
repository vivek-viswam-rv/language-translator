{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# downloading the europarl fr-en dataset\n",
    "url = \"http://www.statmt.org/europarl/v7/fr-en.tgz\"\n",
    "filename = \"fr-en.tgz\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "with tarfile.open(filename, \"r:gz\") as tar:\n",
    "    tar.extractall(\"europarl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c98c758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sentence pairs: 2007723\n"
     ]
    }
   ],
   "source": [
    "ENGLISH_PATH = \"europarl/europarl-v7.fr-en.en\"\n",
    "FRENCH_PATH = \"europarl/europarl-v7.fr-en.fr\"\n",
    "\n",
    "def zip_files(english_path, french_path):\n",
    "    with open(english_path, \"r\", encoding=\"utf-8\") as f_english, \\\n",
    "         open(french_path, \"r\", encoding=\"utf-8\") as f_french:\n",
    "        french_lines = f_french.readlines()\n",
    "        english_lines = f_english.readlines()\n",
    "\n",
    "    assert len(english_lines) == len(french_lines), \"different number of lines in files!!\"\n",
    "    pairs = list(zip(french_lines, english_lines))\n",
    "    return pairs\n",
    "\n",
    "sentence_pairs = zip_files(ENGLISH_PATH, FRENCH_PATH)\n",
    "\n",
    "print(\"total sentence pairs:\", len(sentence_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77940f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after cleaning total sentence pairs: 2002756\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    # normalize unicode to NFC\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    # replace non breaking space with regular space\n",
    "    s = s.replace(\"\\xa0\", \" \")\n",
    "    # collapse multiple spaces into a single space\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # strip whitespace and lowercase all letters\n",
    "    return s.strip().lower()\n",
    "\n",
    "def clean_data(sentences):\n",
    "    cleaned_data = []\n",
    "    for english, french in sentences:\n",
    "        english_cleaned = clean_text(english)\n",
    "        french_cleaned = clean_text(french)\n",
    "        # skip empty sentences\n",
    "        if english_cleaned == \"\" or french_cleaned == \"\":\n",
    "            continue\n",
    "        cleaned_data.append((french_cleaned, english_cleaned))\n",
    "    return cleaned_data\n",
    "\n",
    "cleaned_data = clean_data(sentence_pairs)\n",
    "print(\"after cleaning total sentence pairs:\", len(cleaned_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f053c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after tokenization & length filtering: 1654263\n",
      "example: (['je', 'déclare', 'reprise', 'la', 'session', 'du', 'parlement', 'européen', 'qui', 'avait', 'été', 'interrompue', 'le', 'vendredi', '17', 'décembre', 'dernier', 'et', 'je', 'vous', 'renouvelle', 'tous', 'mes', 'vux', 'en', 'espérant', 'que', 'vous', 'avez', 'passé', 'de', 'bonnes', 'vacances.'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'])\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "def tokenize_and_filter(sentence_pairs, max_len=40, min_len=1):\n",
    "    # tokenize and filter sentences by length\n",
    "    token_pairs = []\n",
    "    for english, french in sentence_pairs:\n",
    "        english_tokens = tokenize(english)\n",
    "        french_tokens = tokenize(french)\n",
    "\n",
    "        if not (min_len <= len(english_tokens) <= max_len):\n",
    "            continue\n",
    "        if not (min_len <= len(french_tokens) <= max_len):\n",
    "            continue\n",
    "\n",
    "        token_pairs.append((french_tokens, english_tokens))\n",
    "    return token_pairs\n",
    "\n",
    "tokenized_pairs = tokenize_and_filter(cleaned_data, max_len=40)\n",
    "print(\"after tokenization & length filtering:\", len(tokenized_pairs))\n",
    "print(\"example:\", tokenized_pairs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train samples: 1323410, number of validation samples: 165426, number of test samples: 165427\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(tokenized_pairs)\n",
    "\n",
    "# split into 80% train, 10% validation, 10% test\n",
    "n_total = len(tokenized_pairs)\n",
    "n_train = int(0.80 * n_total)\n",
    "n_validation = int(0.10 * n_total)\n",
    "n_test = n_total - n_train - n_validation\n",
    "\n",
    "train_pairs = tokenized_pairs[:n_train]\n",
    "validation_pairs = tokenized_pairs[n_train:n_train+n_validation]\n",
    "test_pairs = tokenized_pairs[n_train+n_validation:]\n",
    "\n",
    "print(f\"number of train samples: {len(train_pairs)}, number of validation samples: {len(validation_pairs)}, number of test samples: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07435794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab size (french): 111728\n",
      "target vocab size (english): 79441\n",
      "first 20 source tokens: ['<pad>', '<sos>', '<eos>', '<unk>', 'toutefois,', 'nous', 'sommes', \"d'avis,\", 'avec', 'un', 'grand', 'nombre', 'de', 'blogueurs,', 'que', 'les', 'atteintes', 'à', 'la', 'vie']\n",
      "first 20 target tokens: ['<pad>', '<sos>', '<eos>', '<unk>', 'however,', 'we', 'share', 'the', 'view,', 'with', 'many', 'that', 'violations', 'and', 'slander', 'are', 'equally', 'punishable', 'on', 'as']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "SPECIAL_TOKENS = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "def build_vocab(pairs, lang=\"source\", min_frequency=3):\n",
    "    \"\"\"\n",
    "    pairs: list of (src_tokens, tgt_tokens)\n",
    "    lang: \"src\" for English, \"tgt\" for French\n",
    "    min_freq: minimum times a word must appear to be kept\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for source_tokens, target_tokens in pairs:\n",
    "        tokens = source_tokens if lang == \"source\" else target_tokens\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # keep only tokens that appear at least min_frequency times\n",
    "    itos = SPECIAL_TOKENS + [w for w, c in counter.items() if c >= min_frequency]\n",
    "    stoi = {w: i for i, w in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "source_stoi, source_itos = build_vocab(train_pairs, lang=\"source\", min_frequency=3)  #French\n",
    "target_stoi, target_itos = build_vocab(train_pairs, lang=\"target\", min_frequency=3)  #English\n",
    "\n",
    "print(\"source vocab size (french):\", len(source_itos))\n",
    "print(\"target vocab size (english):\", len(target_itos))\n",
    "print(\"first 20 source tokens:\", source_itos[:20])\n",
    "print(\"first 20 target tokens:\", target_itos[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a14b1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = source_stoi[\"<pad>\"]\n",
    "SOS_IDX = source_stoi[\"<sos>\"]\n",
    "EOS_IDX = source_stoi[\"<eos>\"]\n",
    "UNK_IDX = source_stoi[\"<unk>\"]\n",
    "\n",
    "def tokens_to_ids(tokens, stoi, add_sos_eos=False):\n",
    "    # convert list of tokens to list of ids using the provided stoi mapping\n",
    "    # only add <sos> and <eos> if add_sos_eos is True (for target sequences)\n",
    "    ids = []\n",
    "    if add_sos_eos:\n",
    "        ids.append(stoi[\"<sos>\"])\n",
    "    for token in tokens:\n",
    "        ids.append(stoi.get(token, UNK_IDX))\n",
    "    if add_sos_eos:\n",
    "        ids.append(stoi[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, token_pairs, source_stoi, target_stoi):\n",
    "        self.token_pairs = token_pairs\n",
    "        self.source_stoi = source_stoi\n",
    "        self.target_stoi = target_stoi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_tokens, target_tokens = self.token_pairs[idx]  # (FRENCH, ENGLISH)\n",
    "\n",
    "        # french source, don't add <sos> and <eos>\n",
    "        source_ids = tokens_to_ids(source_tokens, self.source_stoi, add_sos_eos=False)\n",
    "        # english target, add <sos> and <eos>\n",
    "        target_ids = tokens_to_ids(target_tokens, self.target_stoi, add_sos_eos=True)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(source_ids, dtype=torch.long),\n",
    "            torch.tensor(target_ids, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch is the list of (source_sequence, target_sequence) tuples\n",
    "    source_sequences, target_sequences = zip(*batch)\n",
    "\n",
    "    source_lengths = torch.tensor([len(s) for s in source_sequences], dtype=torch.long)\n",
    "    target_lengths = torch.tensor([len(t) for t in target_sequences], dtype=torch.long)\n",
    "\n",
    "    # pad sequences to max sequence length in this batch\n",
    "    source_padded = pad_sequence(source_sequences, batch_first=True, padding_value=PAD_IDX)\n",
    "    target_padded = pad_sequence(target_sequences, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    return source_padded, target_padded, source_lengths, target_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c99237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_batch shape: torch.Size([64, 40])\n",
      "tgt_batch shape: torch.Size([64, 41])\n",
      "src_lens[:5]: tensor([25, 37, 19, 12, 25])\n",
      "tgt_lens[:5]: tensor([27, 41, 19, 15, 33])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = TranslationDataset(train_pairs, source_stoi, target_stoi)\n",
    "val_dataset   = TranslationDataset(validation_pairs, source_stoi, target_stoi)\n",
    "test_dataset  = TranslationDataset(test_pairs, source_stoi, target_stoi)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "source_batch, target_batch, source_lengths, target_lengths = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f41ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "ENCODER_EMBED_DIM = 256  \n",
    "DECODER_EMBED_DIM = 256  \n",
    "HIDDEN_DIM = 512\n",
    "ENCODER_DROPOUT = 0.2\n",
    "DECODER_DROPOUT = 0.2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        # input_dim = size of French vocab (len(source_stoi))\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=PAD_IDX)\n",
    "\n",
    "        # bidirectional GRU: outputs have size 2 * hid_dim\n",
    "        self.gru = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # to turn [fwd; bwd] into a single decoder hidden state\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        src: [batch_size, src_len]  (French token ids)\n",
    "        returns:\n",
    "          enc_outputs: [batch_size, src_len, 2*hid_dim]\n",
    "          dec_init_hidden: [1, batch_size, hid_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) embed tokens\n",
    "        embedded = self.dropout(self.embedding(x))   # [B, src_len, emb_dim]\n",
    "\n",
    "        # 2) run bidirectional GRU\n",
    "        enc_outputs, hidden = self.gru(embedded)\n",
    "        # enc_outputs: [B, src_len, 2*hid_dim]\n",
    "        # hidden: [num_layers * num_directions, B, hid_dim] = [2, B, hid_dim]\n",
    "\n",
    "        # 3) take last forward & backward hidden states\n",
    "        forward_hidden  = hidden[-2]   # [B, hid_dim]\n",
    "        backward_hidden = hidden[-1]   # [B, hid_dim]\n",
    "\n",
    "        # 4) concatenate & map to decoder hidden size\n",
    "        hidden_cat = torch.cat((forward_hidden, backward_hidden), dim=1)  # [B, 2*hid_dim]\n",
    "        dec_init_hidden = torch.tanh(self.fc(hidden_cat)).unsqueeze(0)    # [1, B, hid_dim]\n",
    "\n",
    "        return enc_outputs, dec_init_hidden\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
