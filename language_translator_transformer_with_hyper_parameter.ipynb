{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgnEkaprXi6e"
      },
      "source": [
        "# Language translator: English to French\n",
        "\n",
        "Vivek Viswam R. V. <br>\n",
        "Rachel Messenger"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "R5CIbK63DVsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVQIQ8jbIKuz",
        "outputId": "1942c303-21cd-4e90-be4b-885bb47a8e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "!python -m pip install torchmetrics\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from torchmetrics.text import BLEUScore\n",
        "from itertools import product\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AynL4IUlG-2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9fddef-98bc-4f31-b0cc-38023851b6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constants"
      ],
      "metadata": {
        "id": "mQWBise3DTw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X5wz1uDTfey"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = \"/content/drive/MyDrive/language-translator\"\n",
        "EN_REL_PATH = \"dataset/europarl-v7.fr-en.en\"\n",
        "FR_REL_PATH = \"dataset/europarl-v7.fr-en.fr\"\n",
        "TRAIN_STATS_FILENAME = \"training_stats.pth\"\n",
        "\n",
        "EN_PATH = os.path.join(ROOT_PATH, EN_REL_PATH)\n",
        "FR_PATH = os.path.join(ROOT_PATH, FR_REL_PATH)\n",
        "TRAIN_STATS_PATH = os.path.join(ROOT_PATH, TRAIN_STATS_FILENAME)\n",
        "\n",
        "START_TOKEN = \"<START>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "UNKN_TOKEN = \"<UNKN>\"\n",
        "\n",
        "MAX_SENT_LEN = 100\n",
        "MAX_SEQ_LEN = MAX_SENT_LEN + 2\n",
        "MAX_LINE_COUNT = 200000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom dataset class definitions"
      ],
      "metadata": {
        "id": "IOitEOMSDcr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LangDataset(Dataset):\n",
        "  START_TOKEN = START_TOKEN\n",
        "  END_TOKEN = END_TOKEN\n",
        "  PAD_TOKEN = PAD_TOKEN\n",
        "  UNKN_TOKEN = UNKN_TOKEN\n",
        "  SPECIAL_TOKENS = ( START_TOKEN, END_TOKEN, PAD_TOKEN, UNKN_TOKEN )\n",
        "\n",
        "  def __init__(self, src_path, tgt_path, src_tokenizer, tgt_tokenizer, min_tok_freq=2, max_lines=None, max_sent_len=MAX_SENT_LEN):\n",
        "    self.src = None\n",
        "    self.tgt = None\n",
        "\n",
        "    self.src_path = src_path\n",
        "    self.tgt_path = tgt_path\n",
        "    self.max_sent_len = max_sent_len\n",
        "    self.max_seq_len = max_sent_len + 2\n",
        "    self.min_tok_freq = min_tok_freq\n",
        "\n",
        "    self.src_lines = []\n",
        "    self.tgt_lines = []\n",
        "    self.src_idx2tok = {}\n",
        "    self.src_tok2idx = {}\n",
        "    self.tgt_idx2tok = {}\n",
        "    self.tgt_tok2idx = {}\n",
        "    self.src_vocab_size = 0\n",
        "    self.tgt_vocab_size = 0\n",
        "\n",
        "    self._src_tokenizer = src_tokenizer\n",
        "    self._tgt_tokenizer = tgt_tokenizer\n",
        "    self._max_lines = max_lines\n",
        "\n",
        "\n",
        "    self._load_dataset()\n",
        "    self._preprocess_dataset()\n",
        "    self._create_vocab()\n",
        "    self._tensorify_dataset()\n",
        "\n",
        "  def tokenize_sentence(self, sentence, lang_type=\"src\"):\n",
        "      tokenize = self._src_tokenizer if lang_type == \"src\" else self._tgt_tokenizer\n",
        "      tok2idx = self.src_tok2idx if lang_type == \"src\" else self.tgt_tok2idx\n",
        "      counter = self.src_tok_ctr if lang_type == \"src\" else self.tgt_tok_ctr\n",
        "\n",
        "      tokens = tokenize(sentence)\n",
        "      tokens = [ self.UNKN_TOKEN if counter[tok] < self.min_tok_freq else tok for tok in tokens ]\n",
        "      tokens = [ self.START_TOKEN ] + tokens + [ self.END_TOKEN ]\n",
        "      padding_size = self.max_seq_len - len(tokens)\n",
        "      tokens += [ self.PAD_TOKEN ] * padding_size\n",
        "      tokens = [ tok2idx[tok] for tok in tokens ]\n",
        "\n",
        "      return tokens\n",
        "\n",
        "  def detokenize_tensor(self, tensor, lang_type=\"src\"):\n",
        "      idx2tok = self.src_idx2tok if lang_type == \"src\" else self.tgt_idx2tok\n",
        "      words = [ idx2tok[idx.item()] for idx in tensor ]\n",
        "\n",
        "      return words\n",
        "\n",
        "  def _load_dataset(self):\n",
        "    print(\"Reading source and target files...\")\n",
        "\n",
        "    with open(self.src_path) as src_f, open(self.tgt_path) as tgt_path:\n",
        "      src_lines = src_f.read().splitlines()\n",
        "      tgt_lines = tgt_path.read().splitlines()\n",
        "\n",
        "      self.src_lines = src_lines[:self._max_lines] if self._max_lines else src_lines\n",
        "      self.tgt_lines = tgt_lines[:self._max_lines] if self._max_lines else tgt_lines\n",
        "\n",
        "    print(f\"Reading source files! Read {len(src_lines)} lines. Keeping {len(self.src_lines)} lines.\")\n",
        "\n",
        "  def _preprocess_dataset(self):\n",
        "    print(\"Preprocessing dataset...\")\n",
        "\n",
        "    final_src_lines = []\n",
        "    final_tgt_lines = []\n",
        "    total_lines = len(self.src_lines)\n",
        "\n",
        "    self.src_tok_ctr = Counter()\n",
        "    self.tgt_tok_ctr = Counter()\n",
        "\n",
        "    self._src_tok_set = set()\n",
        "    self._tgt_tok_set = set()\n",
        "\n",
        "    for idx in range(total_lines):\n",
        "      src_line = self.src_lines[idx].lower()\n",
        "      tgt_line = self.tgt_lines[idx].lower()\n",
        "\n",
        "      src_line_toks = self._src_tokenizer(src_line)\n",
        "      tgt_line_toks = self._tgt_tokenizer(tgt_line)\n",
        "\n",
        "      src_line_len = len(src_line_toks)\n",
        "      tgt_line_len = len(tgt_line_toks)\n",
        "\n",
        "      if src_line_len <= self.max_sent_len and tgt_line_len <= self.max_sent_len:\n",
        "        final_src_lines.append(src_line)\n",
        "        final_tgt_lines.append(tgt_line)\n",
        "\n",
        "        self._src_tok_set |= set(src_line_toks)\n",
        "        self._tgt_tok_set |= set(tgt_line_toks)\n",
        "\n",
        "        self.src_tok_ctr.update(src_line_toks)\n",
        "        self.tgt_tok_ctr.update(tgt_line_toks)\n",
        "\n",
        "    self.src_lines, self.tgt_lines = final_src_lines, final_tgt_lines\n",
        "\n",
        "    print(f\"Preprocessing completed! Dropped {total_lines - len(self.src_lines)}/{total_lines} lines!\")\n",
        "\n",
        "  def _create_vocab(self):\n",
        "    print(\"Creating vocabularies...\")\n",
        "\n",
        "    src_tokens = list(self.SPECIAL_TOKENS) + list(self._src_tok_set)\n",
        "    tgt_tokens = list(self.SPECIAL_TOKENS) + list(self._tgt_tok_set)\n",
        "\n",
        "    del self._src_tok_set\n",
        "    del self._tgt_tok_set\n",
        "\n",
        "    for idx, tok in enumerate(src_tokens):\n",
        "      self.src_tok2idx[tok] = idx\n",
        "      self.src_idx2tok[idx] = tok\n",
        "\n",
        "    for idx, tok in enumerate(tgt_tokens):\n",
        "      self.tgt_tok2idx[tok] = idx\n",
        "      self.tgt_idx2tok[idx] = tok\n",
        "\n",
        "    self.src_vocab_size = len(self.src_tok2idx)\n",
        "    self.tgt_vocab_size = len(self.tgt_tok2idx)\n",
        "\n",
        "    print(f\"Created vocabularies! Source vocab size: {self.src_vocab_size} and target vocab size: {self.tgt_vocab_size}\")\n",
        "\n",
        "  def _tensorify_dataset(self):\n",
        "    print(\"Tensorifying dataset...\")\n",
        "\n",
        "    final_src_lines = [ self.tokenize_sentence(line, \"src\") for line in self.src_lines ]\n",
        "    final_tgt_lines = [ self.tokenize_sentence(line, \"tgt\") for line in self.tgt_lines ]\n",
        "\n",
        "    self.src = torch.tensor(final_src_lines)\n",
        "    self.tgt = torch.tensor(final_tgt_lines)\n",
        "\n",
        "    print(\"Dataset tensorified!\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.src[idx], self.tgt[idx]"
      ],
      "metadata": {
        "id": "lFqTteiWVxp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class definitions"
      ],
      "metadata": {
        "id": "DFol28NEDl3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_size,\n",
        "               num_heads,\n",
        "               num_encoder_layers,\n",
        "               num_decoder_layers,\n",
        "               dim_feedforward,\n",
        "               dropout,\n",
        "               src_voc_size,\n",
        "               tgt_voc_size,\n",
        "               src_pad_idx,\n",
        "               tgt_pad_idx,\n",
        "               device,\n",
        "               max_sent_len=MAX_SENT_LEN,\n",
        "    ):\n",
        "\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.device = device\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.tgt_pad_idx = tgt_pad_idx\n",
        "    self.max_seq_len = max_sent_len + 2\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.src_word_embedding = nn.Embedding(src_voc_size, embedding_size)\n",
        "    self.src_position_embedding = nn.Embedding(self.max_seq_len, embedding_size)\n",
        "    self.tgt_word_embedding = nn.Embedding(tgt_voc_size, embedding_size)\n",
        "    self.tgt_position_embedding = nn.Embedding(self.max_seq_len, embedding_size)\n",
        "    self.transformer = nn.Transformer(\n",
        "        embedding_size,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dim_feedforward,\n",
        "        dropout\n",
        "    )\n",
        "\n",
        "    self.fc_out = nn.Linear(embedding_size, tgt_voc_size)\n",
        "\n",
        "  def create_padding_mask(self, seq, pad_idx):\n",
        "    return seq.transpose(0, 1) == pad_idx\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    src_seq_len, batch_size = src.shape\n",
        "    tgt_seq_len, _ = tgt.shape\n",
        "\n",
        "    src_positions = (\n",
        "        torch.arange(0, src_seq_len).unsqueeze(1).expand(src_seq_len, batch_size)\n",
        "        .to(self.device)\n",
        "    )\n",
        "\n",
        "    tgt_positions = (\n",
        "        torch.arange(0, tgt_seq_len).unsqueeze(1).expand(tgt_seq_len, batch_size)\n",
        "        .to(self.device)\n",
        "    )\n",
        "\n",
        "    emb_src = self.dropout(\n",
        "        self.src_word_embedding(src) + self.src_position_embedding(src_positions)\n",
        "    )\n",
        "\n",
        "    emb_tgt = self.dropout(\n",
        "        self.tgt_word_embedding(tgt) + self.tgt_position_embedding(tgt_positions)\n",
        "    )\n",
        "\n",
        "    src_padding_mask = self.create_padding_mask(src, self.src_pad_idx)\n",
        "    tgt_padding_mask = self.create_padding_mask(tgt, self.tgt_pad_idx)\n",
        "    tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len).to(self.device)\n",
        "\n",
        "    out = self.transformer(\n",
        "        emb_src,\n",
        "        emb_tgt,\n",
        "        tgt_mask=tgt_mask,\n",
        "        src_key_padding_mask=src_padding_mask,\n",
        "        tgt_key_padding_mask=tgt_padding_mask\n",
        "    )\n",
        "\n",
        "    return self.fc_out(out)"
      ],
      "metadata": {
        "id": "Kpkl5o8q999f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_translate_tensor(model, batch_data, max_length=MAX_SEQ_LEN, detokenize=True):\n",
        "  data_len, seq_len = batch_data.shape\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  src = batch_data.T.to(device)\n",
        "  tgt = torch.full(\n",
        "      (1, data_len),\n",
        "      dataset.tgt_tok2idx[START_TOKEN],\n",
        "      dtype=torch.long,\n",
        "      device=device\n",
        "  )\n",
        "\n",
        "  finished = torch.zeros(data_len, dtype=torch.bool, device=device)\n",
        "\n",
        "  for _ in range(max_length-1):\n",
        "    with torch.no_grad():\n",
        "      output = model(src, tgt)\n",
        "\n",
        "    next_token_logits = output[-1, :, :]\n",
        "    next_tokens = next_token_logits.argmax(dim=1)\n",
        "\n",
        "    next_tokens = next_tokens.masked_fill(finished, dataset.src_tok2idx[END_TOKEN])\n",
        "    tgt = torch.cat([tgt, next_tokens.unsqueeze(0)], dim=0)\n",
        "\n",
        "    finished |= (next_tokens == dataset.src_tok2idx[END_TOKEN])\n",
        "    if finished.all():\n",
        "      break\n",
        "\n",
        "  if not detokenize:\n",
        "    return tgt.T\n",
        "\n",
        "  translations = []\n",
        "  tgt_np = tgt.detach().cpu().numpy().T\n",
        "\n",
        "  for seq in tgt_np:\n",
        "    words = []\n",
        "    for idx in seq[1:]:\n",
        "      if idx == dataset.tgt_tok2idx[END_TOKEN]:\n",
        "        break\n",
        "      words.append(dataset.tgt_idx2tok[idx])\n",
        "    translations.append(\" \".join(words))\n",
        "\n",
        "  return translations\n",
        "\n",
        "def en_token_rejoin(tokens):\n",
        "  sentence = \"\"\n",
        "  for token in tokens:\n",
        "    if token[0] in [\"'\", \".\", \",\", \"!\", \"?\", \":\", \";\"]:\n",
        "      sentence = sentence.rstrip() + token\n",
        "      continue\n",
        "\n",
        "    sentence+= token + \" \"\n",
        "\n",
        "  return sentence\n",
        "\n",
        "def translate_sentence(model, sentence, max_length=MAX_SENT_LEN):\n",
        "  tokens = [token.text.lower() for token in spacy_fr(sentence)]\n",
        "\n",
        "  tokens.insert(0, START_TOKEN)\n",
        "  tokens.append(END_TOKEN)\n",
        "\n",
        "  text_to_indices = [ dataset.src_tok2idx.get(token, dataset.src_tok2idx[UNKN_TOKEN]) for token in tokens]\n",
        "  sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "  outputs = [dataset.tgt_tok2idx[START_TOKEN]]\n",
        "  for i in range(max_length):\n",
        "      tgt_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output = model(sentence_tensor, tgt_tensor)\n",
        "\n",
        "      best_guess = output.argmax(2)[-1, :].item()\n",
        "      if best_guess == dataset.tgt_tok2idx[END_TOKEN]:\n",
        "          break\n",
        "\n",
        "      outputs.append(best_guess)\n",
        "\n",
        "  translated_sentence = [dataset.tgt_idx2tok[idx] for idx in outputs]\n",
        "\n",
        "  return en_token_rejoin(translated_sentence[1:])\n",
        "\n",
        "def bleu_score(model, test_iter, max_count=None, verbose=True):\n",
        "  tgts = []\n",
        "  pred_tgts = []\n",
        "  cnt = 0\n",
        "\n",
        "  for (x, y) in test_iter:\n",
        "    src = x.to(device)\n",
        "    tgt = y[:, :-1]\n",
        "\n",
        "    pred = batch_translate_tensor(model, src)\n",
        "    pred_tgts.extend(pred)\n",
        "    for sent_tok in tgt:\n",
        "      tgts.append([\" \".join([ val for tok in sent_tok if (val:=dataset.tgt_idx2tok[tok.item()]) not in [START_TOKEN, END_TOKEN, PAD_TOKEN]])])\n",
        "\n",
        "    cnt += src.shape[0]\n",
        "    if max_count and cnt >= max_count:\n",
        "      break\n",
        "\n",
        "  bleu = BLEUScore().to(device)\n",
        "  score = bleu(pred_tgts, tgts)\n",
        "\n",
        "  if(verbose):\n",
        "    print(f\"Total tested datapoints: {cnt}\")\n",
        "    print(f\"BLEU score: {score * 100:.2f}%\")\n",
        "\n",
        "  return score.item()"
      ],
      "metadata": {
        "id": "gzVbYR-dCh8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misc definitions"
      ],
      "metadata": {
        "id": "2Y9twgDBxVSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "tokenize_fr = lambda text: [ text for tok in spacy_fr.tokenizer(text) if (text:=tok.text.strip()) ]\n",
        "tokenize_en = lambda text: [ text for tok in spacy_en.tokenizer(text) if (text:=tok.text.strip()) ]"
      ],
      "metadata": {
        "id": "IN7LG1wNxUWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading dataset and vocabularies"
      ],
      "metadata": {
        "id": "zq3_cbXODoEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqMsi5lk6EKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b129cf9-804c-4385-f75f-edf76ca34f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading source and target files...\n",
            "Reading source files! Read 2007723 lines. Keeping 20000 lines.\n",
            "Preprocessing dataset...\n",
            "Preprocessing completed! Dropped 156/20000 lines!\n",
            "Creating vocabularies...\n",
            "Created vocabularies! Source vocab size: 20151 and target vocab size: 14531\n",
            "Tensorifying dataset...\n",
            "Dataset tensorified!\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024\n",
        "train_test_split = [0.9, 0.1]\n",
        "\n",
        "dataset = LangDataset(FR_PATH, EN_PATH, tokenize_fr, tokenize_en, max_lines=20000)\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(dataset, train_test_split)\n",
        "train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_iter = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning"
      ],
      "metadata": {
        "id": "Zeg7Z1biDzm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_iter, val_iter, dataset, device, config):\n",
        "  embedding_size = config[\"embedding_size\"]\n",
        "  num_heads = config[\"num_heads\"]\n",
        "  num_encoder_layers = config[\"num_encoder_layers\"]\n",
        "  num_decoder_layers = config[\"num_decoder_layers\"]\n",
        "  dropout = config[\"dropout\"]\n",
        "  learning_rate = config[\"learning_rate\"]\n",
        "  num_epochs = config[\"num_epochs\"]\n",
        "\n",
        "  log_per = 5\n",
        "  dim_feedforward = 4 * embedding_size\n",
        "\n",
        "  src_pad_idx = dataset.src_tok2idx[PAD_TOKEN]\n",
        "  tgt_pad_idx = dataset.tgt_tok2idx[PAD_TOKEN]\n",
        "  tgt_start_idx = dataset.tgt_tok2idx[START_TOKEN]\n",
        "  tgt_end_idx = dataset.tgt_tok2idx[END_TOKEN]\n",
        "\n",
        "  model = Transformer(\n",
        "    embedding_size,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    dim_feedforward,\n",
        "    dropout,\n",
        "    dataset.src_vocab_size,\n",
        "    dataset.tgt_vocab_size,\n",
        "    src_pad_idx,\n",
        "    tgt_pad_idx,\n",
        "    device\n",
        "  ).to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
        "\n",
        "  loss_history = []\n",
        "  acc_history = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    batch_loss = []\n",
        "    batch_acc = []\n",
        "\n",
        "    model.train()\n",
        "    for src, target in train_iter:\n",
        "      src = src.to(device).T\n",
        "      target = target.to(device).T\n",
        "\n",
        "      target_input  = target[:-1, :]\n",
        "      target_output = target[1:, :]\n",
        "\n",
        "      output = model(src, target_input)\n",
        "      pred = output.argmax(2)\n",
        "\n",
        "      mask = (\n",
        "        (target_output != tgt_end_idx) &\n",
        "        (target_output != tgt_pad_idx)\n",
        "      )\n",
        "\n",
        "      correct  = (pred == target_output) & mask\n",
        "      accuracy = correct.sum() / mask.sum()\n",
        "      batch_acc.append(accuracy.item())\n",
        "\n",
        "      output_flat = output.reshape(-1, output.shape[2])\n",
        "      target_flat = target_output.reshape(-1)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(output_flat, target_flat)\n",
        "      loss.backward()\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "      optimizer.step()\n",
        "\n",
        "    avg_acc  = sum(batch_acc) / len(batch_acc)\n",
        "    avg_loss = sum(batch_loss) / len(batch_loss)\n",
        "    acc_history.append(avg_acc)\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    if epoch % log_per == 0:\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} - loss: {avg_loss:.4f} - train acc: {avg_acc * 100:.2f}%\")\n",
        "\n",
        "  test_bleu = bleu_score(model, test_iter, verbose=False)\n",
        "  print(f\"Epoch {epoch + 1}/{num_epochs} - loss: {avg_loss:.4f} - train acc: {avg_acc * 100:.2f}%\")\n",
        "  print(f\"Test BLEU: {test_bleu * 100:.2f}%\")\n",
        "\n",
        "  return test_bleu, model.state_dict(), loss_history, acc_history"
      ],
      "metadata": {
        "id": "AB4yrRVK-mth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "  \"embedding_size\": [256, 512],\n",
        "  \"num_heads\": [4, 8],\n",
        "  \"num_encoder_layers\": [2, 3],\n",
        "  \"num_decoder_layers\": [2, 3],\n",
        "  \"dropout\": [0.1, 0.2],\n",
        "  \"learning_rate\": [3e-4],\n",
        "  \"num_epochs\":[20]\n",
        "}\n",
        "\n",
        "def param_combinations(grid):\n",
        "  keys = list(grid.keys())\n",
        "  for values in product(*grid.values()):\n",
        "    yield dict(zip(keys, values))\n",
        "\n",
        "best_config = None\n",
        "best_bleu = -1.0\n",
        "best_state_dict = None\n",
        "\n",
        "for cfg in param_combinations(param_grid):\n",
        "  print(\"\\n==============================\")\n",
        "  print(\"Config:\", cfg)\n",
        "  print(\"==============================\")\n",
        "\n",
        "  test_bleu, state_dict, loss_hist, acc_hist = train(\n",
        "    train_iter, test_iter, dataset, device, cfg\n",
        "  )\n",
        "\n",
        "  if test_bleu > best_bleu:\n",
        "    best_bleu = test_bleu\n",
        "    best_config = cfg\n",
        "    best_state_dict = state_dict\n",
        "\n",
        "print(\"\\nBest config found:\")\n",
        "print(best_config)\n",
        "print(f\"Best test BLEU: {best_bleu * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp-K4nQG_DkP",
        "outputId": "67737326-44a9-47b1-dd87-0ccd257e96e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.3006 - train acc: 6.20%\n",
            "Epoch 6/20 - loss: 5.4595 - train acc: 16.15%\n",
            "Epoch 11/20 - loss: 4.9353 - train acc: 20.70%\n",
            "Epoch 16/20 - loss: 4.5932 - train acc: 24.12%\n",
            "Epoch 20/20 - loss: 4.3725 - train acc: 26.21%\n",
            "Test BLEU: 4.59%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.3917 - train acc: 5.89%\n",
            "Epoch 6/20 - loss: 5.6242 - train acc: 13.95%\n",
            "Epoch 11/20 - loss: 5.1235 - train acc: 18.26%\n",
            "Epoch 16/20 - loss: 4.8190 - train acc: 21.01%\n",
            "Epoch 20/20 - loss: 4.6310 - train acc: 22.72%\n",
            "Test BLEU: 3.68%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.1889 - train acc: 6.37%\n",
            "Epoch 6/20 - loss: 5.4338 - train acc: 16.67%\n",
            "Epoch 11/20 - loss: 4.9054 - train acc: 20.97%\n",
            "Epoch 16/20 - loss: 4.5632 - train acc: 24.38%\n",
            "Epoch 20/20 - loss: 4.3387 - train acc: 26.59%\n",
            "Test BLEU: 5.02%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2881 - train acc: 6.13%\n",
            "Epoch 6/20 - loss: 5.5802 - train acc: 14.61%\n",
            "Epoch 11/20 - loss: 5.0790 - train acc: 18.83%\n",
            "Epoch 16/20 - loss: 4.7880 - train acc: 21.37%\n",
            "Epoch 20/20 - loss: 4.6015 - train acc: 23.08%\n",
            "Test BLEU: 3.57%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.1791 - train acc: 6.39%\n",
            "Epoch 6/20 - loss: 5.4709 - train acc: 15.96%\n",
            "Epoch 11/20 - loss: 4.9643 - train acc: 20.21%\n",
            "Epoch 16/20 - loss: 4.6282 - train acc: 23.44%\n",
            "Epoch 20/20 - loss: 4.4061 - train acc: 25.63%\n",
            "Test BLEU: 4.23%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.3478 - train acc: 6.33%\n",
            "Epoch 6/20 - loss: 5.6148 - train acc: 14.03%\n",
            "Epoch 11/20 - loss: 5.1397 - train acc: 18.04%\n",
            "Epoch 16/20 - loss: 4.8535 - train acc: 20.43%\n",
            "Epoch 20/20 - loss: 4.6722 - train acc: 22.10%\n",
            "Test BLEU: 3.38%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2202 - train acc: 5.93%\n",
            "Epoch 6/20 - loss: 5.4451 - train acc: 16.59%\n",
            "Epoch 11/20 - loss: 4.9214 - train acc: 20.59%\n",
            "Epoch 16/20 - loss: 4.5996 - train acc: 23.54%\n",
            "Epoch 20/20 - loss: 4.3884 - train acc: 25.50%\n",
            "Test BLEU: 4.35%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2711 - train acc: 6.40%\n",
            "Epoch 6/20 - loss: 5.5949 - train acc: 14.49%\n",
            "Epoch 11/20 - loss: 5.0989 - train acc: 18.52%\n",
            "Epoch 16/20 - loss: 4.8175 - train acc: 20.77%\n",
            "Epoch 20/20 - loss: 4.6375 - train acc: 22.50%\n",
            "Test BLEU: 3.20%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2546 - train acc: 6.31%\n",
            "Epoch 6/20 - loss: 5.4284 - train acc: 16.45%\n",
            "Epoch 11/20 - loss: 4.9214 - train acc: 20.50%\n",
            "Epoch 16/20 - loss: 4.6033 - train acc: 23.61%\n",
            "Epoch 20/20 - loss: 4.3868 - train acc: 25.81%\n",
            "Test BLEU: 4.17%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.3096 - train acc: 6.17%\n",
            "Epoch 6/20 - loss: 5.5901 - train acc: 14.45%\n",
            "Epoch 11/20 - loss: 5.0963 - train acc: 18.51%\n",
            "Epoch 16/20 - loss: 4.8114 - train acc: 20.93%\n",
            "Epoch 20/20 - loss: 4.6327 - train acc: 22.47%\n",
            "Test BLEU: 3.21%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2208 - train acc: 6.42%\n",
            "Epoch 6/20 - loss: 5.4076 - train acc: 16.72%\n",
            "Epoch 11/20 - loss: 4.8965 - train acc: 20.97%\n",
            "Epoch 16/20 - loss: 4.5731 - train acc: 24.03%\n",
            "Epoch 20/20 - loss: 4.3531 - train acc: 26.25%\n",
            "Test BLEU: 3.99%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.3304 - train acc: 6.05%\n",
            "Epoch 6/20 - loss: 5.5897 - train acc: 14.70%\n",
            "Epoch 11/20 - loss: 5.0790 - train acc: 18.68%\n",
            "Epoch 16/20 - loss: 4.7965 - train acc: 21.07%\n",
            "Epoch 20/20 - loss: 4.6167 - train acc: 22.63%\n",
            "Test BLEU: 3.12%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2756 - train acc: 6.31%\n",
            "Epoch 6/20 - loss: 5.4564 - train acc: 16.13%\n",
            "Epoch 11/20 - loss: 4.9439 - train acc: 20.24%\n",
            "Epoch 16/20 - loss: 4.6312 - train acc: 23.07%\n",
            "Epoch 20/20 - loss: 4.4237 - train acc: 25.05%\n",
            "Test BLEU: 4.11%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2865 - train acc: 6.20%\n",
            "Epoch 6/20 - loss: 5.5912 - train acc: 14.20%\n",
            "Epoch 11/20 - loss: 5.1127 - train acc: 18.21%\n",
            "Epoch 16/20 - loss: 4.8384 - train acc: 20.37%\n",
            "Epoch 20/20 - loss: 4.6633 - train acc: 22.04%\n",
            "Test BLEU: 2.86%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2633 - train acc: 6.40%\n",
            "Epoch 6/20 - loss: 5.4327 - train acc: 16.46%\n",
            "Epoch 11/20 - loss: 4.9199 - train acc: 20.52%\n",
            "Epoch 16/20 - loss: 4.5969 - train acc: 23.48%\n",
            "Epoch 20/20 - loss: 4.3827 - train acc: 25.69%\n",
            "Test BLEU: 4.09%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 256, 'num_heads': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 8.2419 - train acc: 6.38%\n",
            "Epoch 6/20 - loss: 5.5778 - train acc: 14.65%\n",
            "Epoch 11/20 - loss: 5.0884 - train acc: 18.65%\n",
            "Epoch 16/20 - loss: 4.8099 - train acc: 20.88%\n",
            "Epoch 20/20 - loss: 4.6353 - train acc: 22.34%\n",
            "Test BLEU: 3.34%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4199 - train acc: 6.78%\n",
            "Epoch 6/20 - loss: 4.8916 - train acc: 20.98%\n",
            "Epoch 11/20 - loss: 4.2576 - train acc: 27.72%\n",
            "Epoch 16/20 - loss: 3.7687 - train acc: 32.84%\n",
            "Epoch 20/20 - loss: 3.4200 - train acc: 36.65%\n",
            "Test BLEU: 9.46%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4423 - train acc: 6.63%\n",
            "Epoch 6/20 - loss: 5.0527 - train acc: 19.07%\n",
            "Epoch 11/20 - loss: 4.5312 - train acc: 24.02%\n",
            "Epoch 16/20 - loss: 4.1403 - train acc: 27.91%\n",
            "Epoch 20/20 - loss: 3.8705 - train acc: 30.46%\n",
            "Test BLEU: 6.23%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4663 - train acc: 6.53%\n",
            "Epoch 6/20 - loss: 4.9006 - train acc: 21.10%\n",
            "Epoch 11/20 - loss: 4.2207 - train acc: 28.24%\n",
            "Epoch 16/20 - loss: 3.7060 - train acc: 33.62%\n",
            "Epoch 20/20 - loss: 3.3549 - train acc: 37.45%\n",
            "Test BLEU: 10.52%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.5147 - train acc: 6.44%\n",
            "Epoch 6/20 - loss: 5.0911 - train acc: 19.08%\n",
            "Epoch 11/20 - loss: 4.5294 - train acc: 24.23%\n",
            "Epoch 16/20 - loss: 4.1187 - train acc: 28.32%\n",
            "Epoch 20/20 - loss: 3.8424 - train acc: 31.01%\n",
            "Test BLEU: 6.69%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.3976 - train acc: 6.53%\n",
            "Epoch 6/20 - loss: 4.9432 - train acc: 20.17%\n",
            "Epoch 11/20 - loss: 4.3442 - train acc: 26.24%\n",
            "Epoch 16/20 - loss: 3.8476 - train acc: 31.57%\n",
            "Epoch 20/20 - loss: 3.5012 - train acc: 35.29%\n",
            "Test BLEU: 7.67%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.5084 - train acc: 6.59%\n",
            "Epoch 6/20 - loss: 5.0947 - train acc: 18.56%\n",
            "Epoch 11/20 - loss: 4.5801 - train acc: 23.23%\n",
            "Epoch 16/20 - loss: 4.1977 - train acc: 27.05%\n",
            "Epoch 20/20 - loss: 3.9299 - train acc: 29.65%\n",
            "Test BLEU: 6.13%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4202 - train acc: 6.50%\n",
            "Epoch 6/20 - loss: 4.9620 - train acc: 20.06%\n",
            "Epoch 11/20 - loss: 4.3366 - train acc: 26.40%\n",
            "Epoch 16/20 - loss: 3.8297 - train acc: 31.87%\n",
            "Epoch 20/20 - loss: 3.4632 - train acc: 35.94%\n",
            "Test BLEU: 8.19%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.5325 - train acc: 6.39%\n",
            "Epoch 6/20 - loss: 5.1152 - train acc: 18.47%\n",
            "Epoch 11/20 - loss: 4.5793 - train acc: 23.28%\n",
            "Epoch 16/20 - loss: 4.1979 - train acc: 27.03%\n",
            "Epoch 20/20 - loss: 3.9146 - train acc: 29.87%\n",
            "Test BLEU: 5.93%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4124 - train acc: 6.84%\n",
            "Epoch 6/20 - loss: 4.8942 - train acc: 21.00%\n",
            "Epoch 11/20 - loss: 4.2753 - train acc: 27.24%\n",
            "Epoch 16/20 - loss: 3.7791 - train acc: 32.46%\n",
            "Epoch 20/20 - loss: 3.4394 - train acc: 36.05%\n",
            "Test BLEU: 8.64%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4917 - train acc: 6.42%\n",
            "Epoch 6/20 - loss: 5.0701 - train acc: 18.92%\n",
            "Epoch 11/20 - loss: 4.5538 - train acc: 23.56%\n",
            "Epoch 16/20 - loss: 4.1680 - train acc: 27.40%\n",
            "Epoch 20/20 - loss: 3.9043 - train acc: 29.91%\n",
            "Test BLEU: 6.45%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4520 - train acc: 6.59%\n",
            "Epoch 6/20 - loss: 4.8932 - train acc: 21.00%\n",
            "Epoch 11/20 - loss: 4.2505 - train acc: 27.68%\n",
            "Epoch 16/20 - loss: 3.7342 - train acc: 33.09%\n",
            "Epoch 20/20 - loss: 3.3887 - train acc: 36.83%\n",
            "Test BLEU: 9.15%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 8, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.5080 - train acc: 6.43%\n",
            "Epoch 6/20 - loss: 5.0693 - train acc: 19.21%\n",
            "Epoch 11/20 - loss: 4.5395 - train acc: 23.93%\n",
            "Epoch 16/20 - loss: 4.1419 - train acc: 27.84%\n",
            "Epoch 20/20 - loss: 3.8710 - train acc: 30.41%\n",
            "Test BLEU: 6.44%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 2, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4392 - train acc: 6.71%\n",
            "Epoch 6/20 - loss: 4.9431 - train acc: 20.19%\n",
            "Epoch 11/20 - loss: 4.3579 - train acc: 25.87%\n",
            "Epoch 16/20 - loss: 3.8821 - train acc: 30.90%\n",
            "Epoch 20/20 - loss: 3.5351 - train acc: 34.57%\n",
            "Test BLEU: 8.01%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 2, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.5257 - train acc: 6.49%\n",
            "Epoch 6/20 - loss: 5.0830 - train acc: 18.46%\n",
            "Epoch 11/20 - loss: 4.5778 - train acc: 23.18%\n",
            "Epoch 16/20 - loss: 4.2097 - train acc: 26.67%\n",
            "Epoch 20/20 - loss: 3.9497 - train acc: 29.21%\n",
            "Test BLEU: 5.61%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.4282 - train acc: 6.51%\n",
            "Epoch 6/20 - loss: 4.9511 - train acc: 20.26%\n",
            "Epoch 11/20 - loss: 4.3546 - train acc: 26.06%\n",
            "Epoch 16/20 - loss: 3.8688 - train acc: 31.09%\n",
            "Epoch 20/20 - loss: 3.5148 - train acc: 34.99%\n",
            "Test BLEU: 6.85%\n",
            "\n",
            "==============================\n",
            "Config: {'embedding_size': 512, 'num_heads': 8, 'num_encoder_layers': 3, 'num_decoder_layers': 3, 'dropout': 0.2, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "==============================\n",
            "Epoch 1/20 - loss: 7.5503 - train acc: 6.42%\n",
            "Epoch 6/20 - loss: 5.0954 - train acc: 18.60%\n",
            "Epoch 11/20 - loss: 4.5778 - train acc: 23.26%\n",
            "Epoch 16/20 - loss: 4.2018 - train acc: 26.83%\n",
            "Epoch 20/20 - loss: 3.9356 - train acc: 29.40%\n",
            "Test BLEU: 5.79%\n",
            "\n",
            "Best config found:\n",
            "{'embedding_size': 512, 'num_heads': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dropout': 0.1, 'learning_rate': 0.0003, 'num_epochs': 20}\n",
            "Best test BLEU: 10.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data = torch.load(TRAIN_STATS_PATH)\n",
        "model_state_dict = saved_data[\"model_state_dict\"]\n",
        "\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.10\n",
        "dim_feedforward = 4 * embedding_size\n",
        "src_pad_idx = dataset.src_tok2idx[PAD_TOKEN]\n",
        "tgt_pad_idx = dataset.tgt_tok2idx[PAD_TOKEN]\n",
        "\n",
        "model = Transformer(\n",
        "    embedding_size,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    dim_feedforward,\n",
        "    dropout,\n",
        "    dataset.src_vocab_size,\n",
        "    dataset.tgt_vocab_size,\n",
        "    src_pad_idx,\n",
        "    tgt_pad_idx,\n",
        "    device\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(model_state_dict)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "vhvNCfmEES7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "learning_rate = 3e-4\n",
        "\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.10\n",
        "dim_feedforward = 4 * embedding_size\n",
        "src_pad_idx = dataset.src_tok2idx[PAD_TOKEN]\n",
        "tgt_pad_idx = dataset.tgt_tok2idx[PAD_TOKEN]\n",
        "tgt_start_idx = dataset.tgt_tok2idx[START_TOKEN]\n",
        "tgt_end_idx = dataset.tgt_tok2idx[END_TOKEN]\n",
        "\n",
        "model = Transformer(\n",
        "    embedding_size,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    dim_feedforward,\n",
        "    dropout,\n",
        "    dataset.src_vocab_size,\n",
        "    dataset.tgt_vocab_size,\n",
        "    src_pad_idx,\n",
        "    tgt_pad_idx,\n",
        "    device\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
        "\n",
        "loss_history = []\n",
        "acc_history = []\n",
        "val_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  batch_loss = []\n",
        "  batch_acc = []\n",
        "\n",
        "  model.train()\n",
        "  for src, target in val_iter:\n",
        "    src = src.to(device).T\n",
        "    target = target.to(device).T\n",
        "    target_input = target[:-1, :]\n",
        "    target_output = target[1:, :]\n",
        "\n",
        "    output = model(src, target_input)\n",
        "\n",
        "    pred = output.argmax(2)\n",
        "    mask = (target_output != tgt_end_idx) & (target_output != tgt_pad_idx)\n",
        "    correct = (pred == target_output) & mask\n",
        "    accuracy = correct.sum() / mask.sum()\n",
        "    batch_acc.append(accuracy)\n",
        "\n",
        "    output = output.reshape(-1, output.shape[2])\n",
        "    target = target_output.reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    batch_loss.append(loss.item())\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  avg_batch_acc = float(sum(batch_acc) / len(batch_acc))\n",
        "  avg_batch_loss = sum(batch_loss) / len(batch_loss)\n",
        "  acc_history.append(avg_batch_acc)\n",
        "  loss_history.append(avg_batch_loss)\n",
        "\n",
        "  torch.save({\n",
        "      \"loss_history\": loss_history,\n",
        "      \"acc_history\": acc_history,\n",
        "      \"epoch\": epoch,\n",
        "      \"datapoints\": MAX_LINE_COUNT,\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      }, TRAIN_STATS_PATH)\n",
        "\n",
        "  print(f\"Epoch {epoch + 1} / {num_epochs} - loss: {avg_batch_loss:.4f} - train acc: {avg_batch_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "osEUO2pBL5Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "should_validate = epoch % validation_epoch == 0\n",
        "  if should_validate:\n",
        "    val_cnt = 0\n",
        "    avg_val_acc = 0\n",
        "    for src, target in val_iter:\n",
        "      src = src.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      pred = batch_translate_tensor(model, src, detokenize=False)\n",
        "      pad_len = MAX_SEQ_LEN - pred.shape[1]\n",
        "      padding = torch.full(\n",
        "          (target.shape[0], pad_len),\n",
        "          tgt_pad_idx,\n",
        "          device=device\n",
        "          )\n",
        "      pred = torch.hstack([pred, padding])\n",
        "      mask = (target != tgt_start_idx) & (target != tgt_end_idx) & (target != tgt_pad_idx)\n",
        "      correct = (pred == target) & mask\n",
        "      accuracy = correct.sum()\n",
        "      avg_val_acc += accuracy.item()\n",
        "      val_cnt += mask.sum()\n",
        "\n",
        "    avg_val_acc /= val_cnt\n",
        "    val_history.append(avg_val_acc)\n",
        "\n",
        "  avg_batch_acc = float(sum(batch_acc) / len(batch_acc))\n",
        "  avg_batch_loss = sum(batch_loss) / len(batch_loss)\n",
        "  acc_history.append(avg_batch_acc)\n",
        "  loss_history.append(avg_batch_loss)\n",
        "\n",
        "  torch.save({\n",
        "      \"loss_history\": loss_history,\n",
        "      \"acc_history\": acc_history,\n",
        "      \"val_history\": val_history,\n",
        "      \"epoch\": epoch,\n",
        "      \"datapoints\": MAX_LINE_COUNT,\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      }, TRAIN_STATS_PATH)\n",
        "\n",
        "  print_statement = f\"Epoch {epoch + 1} / {num_epochs} - loss: {avg_batch_loss:.4f} - train acc: {avg_batch_acc * 100:.2f}%\"\n",
        "\n",
        "  print(print_statement, f\"val acc: {avg_val_acc * 100:.2f}%\") if should_validate else print(print_statement)"
      ],
      "metadata": {
        "id": "ZzJnEdjP1EHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aGvP5oDgZnm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = \"je suis Vivek\"\n",
        "actual_translation = \"I am in the process of\"\n",
        "model.eval()\n",
        "prediction = translate_sentence(model, input_sent)\n",
        "print(f\"Input sentence: {input_sent}\")\n",
        "print(f\"Translated sentence: {prediction}\")\n",
        "print(f\"Actual translation: {actual_translation}\")"
      ],
      "metadata": {
        "id": "D-XQ61gotSS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "bleu_score(model, test_iter)"
      ],
      "metadata": {
        "id": "8KbjwCRWuFg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wim-PkS1SFj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}